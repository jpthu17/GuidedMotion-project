<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Peng Jin<sup>1,2,3</sup>, </span>
            <span class="author-block">Hao Li<sup>1,2,3</sup>, </span>
            <span class="author-block">Zesen Cheng<sup>1,3</sup>, </span>
            <span class="author-block">Kehan Li<sup>1,3</sup>, </span> <br />
            <span class="author-block">Runyi Yu<sup>1,3</sup>, </span>
            <span class="author-block">Chang Liu<sup>4</sup>, </span>
            <span class="author-block">Xiangyang Ji<sup>4</sup>, </span>
            <span class="author-block">Li Yuan<sup>1,2,3</sup>, </span>
            <span class="author-block">Jie Chen<sup>1,2,3</sup>, </span>
          </div>

          <div class="is-size-8 publication-authors">
            <span class="author-block"><sup>1</sup>School of Electronic and Computer Engineering, Peking University </span> <br />
            <span class="author-block"><sup>2</sup>Peng Cheng Laboratory </span> <br />
            <span class="author-block"><sup>3</sup>AI for Science (AI4S)-Preferred Program, Peking University </span> <br />
            <span class="author-block"><sup>4</sup>Department of Automation and BNRist, Tsinghua University </span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="./static/images/fig1.png"
                 class="column is-centered has-text-centered"
                 alt="Teaser image."/>

      <!-- <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video> -->
      <h2 class="subtitle has-text-centered">
        <b> Our method (GuidedMotion) </b> empowers users to combine preferred local actions freely, generating motions that align with their mental imagery.
      </h2>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Text-to-motion generation requires not only grounding local actions in language but also seamlessly blending these individual actions to synthesize diverse and realistic global motions.
            However, existing motion generation methods primarily focus on the direct synthesis of global motions while neglecting the importance of generating and controlling local actions.
            In this paper, we propose the local action-guided motion diffusion model, which facilitates global motion generation by utilizing local actions as fine-grained control signals.
            Specifically, we provide an automated method for reference local action sampling and leverage graph attention networks to assess the guiding weight of each local action in the overall motion synthesis.
            During the diffusion process for synthesizing global motion, we calculate the local-action gradient to provide conditional guidance.
            This local-to-global paradigm reduces the complexity associated with direct global motion generation and promotes motion diversity via sampling diverse actions as conditions.
            Extensive experiments on two human motion datasets, \ie, HumanML3D and KIT, demonstrate the effectiveness of our method.
            Furthermore, our method provides flexibility in seamlessly combining various local actions and continuous guiding weight adjustment, accommodating diverse user preferences, which may hold potential significance for the community.
          </p>
        </div>
      </div>
    </div>

    <!-- Paper video. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="col-md-4 col-12 paper_img"> <video
                src="./static/images/video.mp4" width="100%"
                playsinline="" autoplay="autoplay" loop="loop" preload="" muted="" alt="Popup Image" class="img-fluid">
            </div>
      </div>
    </div>
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Summary -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Summary</h2>
        <div class="content has-text-justified">
          <h4>Problems</h4>
          <ul>
            <li>Existing text-to-motion generation methods primarily focus on directly synthesizing global motions based on language instructions. However, they come with limitations regarding the type of control they support over the motion results.</li>
            <li>Typically, generating a motion that faithfully corresponds to our mental imagery requires numerous iterations of editing a prompt, reviewing the resulting motion, and then adjusting the prompt accordingly.</li>
          </ul>
          <h4>Our Solutions</h4>
          <ul>
            <li>We propose to employ reference local actions as control signals in the global motion generation process.</li>
            <li>These reference local actions can serve as control signals during the global motion generation process, facilitating the generation of global motions with similar characteristics, including movement trajectories and human body postures, to those of the local actions.</li>
            <li>Users can seamlessly combine their preferred local actions, exerting precise control over the resulting global motion to align with the characteristics of those chosen local actions.</li>
          </ul>
          </p>
        </div>

      </div>
    </div>

    <!-- Pipeline -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Our Pipeline</h2>
        <div class="content has-text-justified">
          <p>
            In GuidedMotion, we provide an automatic local action sampling method, which deconstructs the
            original motion description into multiple local action descriptions and uses a
             text-to-motion model to generate the reference local actions. Subsequently, we leverage graph attention networks to
             estimate the guiding weight of each local motion in the overall motion synthesis.
            To enhance generation stability, we divide the motion diffusion process for
           synthesizing global motion into three stages:
            <li> In the initial diffusion stage, we de-noise the Gaussian noise based on the original motion description to provide
            a good initial value for the subsequent stage.</li>
            <li> In the second diffusion stage, we apply local-action gradients based on the energy function to offer
          conditional guidance for aligning the generated motion with the characteristics
            of the reference local actions.</li>
            <li> In the final diffusion stage, we fine-tune the generated results further to conform to the original motion description, rather
          than solely adhering to a reference local action.</li>
          </p>
        </div>
        <figure  class="content has-text-centered">
          <img src="./static/images/fig2.png"
                 class="column is-centered has-text-centered"
                 alt="Method pipeline."
                 />
        </figure >
      </div>
    </div>




  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@inproceedings{guidedmotion,
  title={Local Action-Guided Motion Diffusion Model for Text-to-Motion Generation},
  author={Jin, Peng and Li, Hao and Cheng, Zesen and Li, Kehan and Yu, Runyi and Liu, Chang and Ji, Xiangyang and Yuan, Li and Chen, jie},
  booktitle={ECCV},
  year={2024}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            The source code is based on the <a href="https://nerfies.github.io/">Nerfies</a> project page. We thank the authors for creating and opening it.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
